---
permalink: /INRV/
excerpt: "INR-V: A Continuous Representation Space for Video-based Generative Tasks"
author_profile: false
---


<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="INR-V: A Continuous Representation Space for Video-based Generative Tasks">
  <meta name="keywords" content="SCARP">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>INR-V: A Continuous Representation Space for Video-based Generative Tasks</title>
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-XQ92MBWJ25"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag() { dataLayer.push(arguments); }
    gtag('js', new Date());

    gtag('config', 'G-XQ92MBWJ25');
    document.title = "INR-V: A Continuous Representation Space for Video-based Generative Tasks";
  </script>

  <link rel="stylesheet" href="/static/css/bulma.min.css">
  <link rel="stylesheet" href="/static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="/static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="/static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="/static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="/static/js/fontawesome.all.min.js"></script>
  <script src="/static/js/bulma-carousel.min.js"></script>
  <script src="/static/js/bulma-slider.min.js"></script>
  <script src="/static/js/index.js"></script>
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js"></script>
  <style>
    .masthead, .page__footer, .page__meta {display: none;}
    .section {padding: 0}
    .overview-section {padding: 0 1.5rem 3rem}
    .is-size-5 {margin-bottom: 10px}
    .title.is-1 {font-size: 45px; font-weight: 800; line-height: 1.2;}
    .underline {text-decoration: underline}
    .more-results { width: 70%!important; }
    .abstract {font-size: 14px!important; line-height: 20px!important; width: 82%!important; margin: auto!important;}
    .small-heading {font-weight: 800!important; font-size: 25px!important; margin-bottom: 25px!important; margin-top: 10px; border-top: none!important; text-decoration: none!important;}
    .headings-scarp {font-family: initial; font-weight: 400!important; font-size: 40px!important; margin-bottom: 25px!important; border-top: none!important; text-decoration: none!important;}
    .page__content h2 {margin-top: 35px;color: black!important;}
    .citation-section {width: 800px;margin: auto;font-size: 13.81px;}
    .small-links {text-decoration: none;}
    .external-link .icon {margin-top: 2px;}
    .external-link {text-decoration: none!important;}
    body {background: #fefcfe;}
    .interpolation-top {position: relative;z-index: 1;}
    .interpolation-bottom {margin-top: -120px;}
    code {font-size:100%}
    .page {width: inherit;
      float: inherit;
      margin-right: inherit;
      padding-left: inherit;
      padding-right: inherit;}
    .ack-scarp {
      margin-bottom: 10px!important;
      margin-top: 40px;
      text-align: center;
    }
    .qual-results {
      margin-bottom: 30px;
    } 
    .iframe-center {
      width: 80%;
      height: 0;
      padding-top: 42.2%;
      margin: auto;
      position: relative;
    }
    iframe {
      width: 100%;
      height: 100%;
      position: absolute;
      top: 0;
      bottom: 0;
      right: 0;
      left: 0;
    } 
  </style>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
	  <!-- <h1 class="title is-1 publication-title">SCARP: 3D <span class="underline">S</span>hape <span class="underline">C</span>ompletion in <span class="underline">AR</span>bitrary <span class="underline">P</span>oses for Improved Grasping</h1> -->
      <h1 class="title is-1 publication-title">INR-V: A Continuous Representation Space for Video-based Generative Tasks</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://scholar.google.co.in/citations?user=GZZCH-8AAAAJ&hl=en">Bipasha Sen</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://scholar.google.co.in/citations?user=64Cgbv4AAAAJ&hl=en">Aditya Agarwal</a><sup>*1</sup>,</span>
            <span class="author-block">
              <a href="https://vinaypn.github.io/">Vinay P Namboodiri</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=U9dH-DoAAAAJ&hl=en">C V Jawahar</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>IIIT Hyderabad, India</span>
            </br>
            <span class="author-block"><sup>2</sup>University of Bath, UK</span>
            </br>
          </div>
          <div>* indicates equal contribution</div>
          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://openreview.net/pdf?id=aIoEkwc2oB"
                   class="small-links external-link button is-normal is-rounded is-dark" target="_blank">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=ViIwnu5vcck"
                   class="small-links external-link button is-normal is-rounded is-dark" target="_blank">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/bipashasen/INRV"
                   class="small-links external-link button is-normal is-rounded is-dark" target="_blank">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <!-- OpenReview page -->
              <span class="link-block">
                <a href="https://openreview.net/forum?id=aIoEkwc2oB"
                   class="small-links external-link button is-normal is-rounded is-dark" target="_blank">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>OpenReview</span>
                </a>
              </span>

              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://"
                   disabled tabindex="-1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
              
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
  <!-- <div>* indicates equal contribution</div> -->
</section>

<section class="hero teaser">
  <center class="iframe-center">
    <!-- <iframe width="850" height="478" src="https://youtu.be/fU2S5rbl_dg" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
    <!--<iframe src="https://www.youtube.com/embed/fU2S5rbl_dg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
    <!-- <iframe src="https://www.youtube.com/embed/0uxWTLQW3HI" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe> -->
    <iframe width="560" height="315" src="https://www.youtube.com/embed/ViIwnu5vcck" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
   </center>
</section>

<br>
<br>
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="headings-scarp">Abstract</h2>
        <div class="columns is-centered has-text-centered">
          <div class="column is-full">
            <img src="/images/projects/inr-v_tmlr2022/banner.png" width="50%" />
          </div>
        </div>
        <div class="content has-text-justified">
          <!-- <p class="content has-text-justified">
            Progress in 3D object understanding has relied on manually "<i>canonicalized</i>" shape datasets that contain instances with
            consistent position and orientation (3D pose). This has made it hard to generalize these methods to in-the-wild shapes, eg., from internet model collections or depth sensors. <b>ConDor</b> is a self-supervised method that learns to <b><u>C</u></b>an<b><u>on</u></b>icalize the 3<b><u>D or</u></b>ientation and position for full and partial 3D point clouds. We build on top of Tensor Field Networks (TFNs), a class of permutation- and rotation-equivariant, and
            translation-invariant 3D networks. During inference, our method takes an unseen full or partial 3D point cloud at an arbitrary pose and outputs an equivariant canonical pose. During training, this network uses self-supervision losses to learn the canonical pose from an un-canonicalized collection of full and partial 3D point clouds.
            <b>ConDor</b> can also learn to consistently co-segment object parts without any supervision. Extensive quantitative results on four new metrics show that our approach outperforms existing methods while enabling new applications such as operation on depth images and annotation transfer.
          </p> -->
          <p class="abstract content has-text-justified">
            Generating videos is a complex task that is accomplished by generating a set of 
            temporally coherent images frame-by-frame. This limits the expressivity of videos 
            to only image-based operations on the individual video frames needing network designs 
            to obtain temporally coherent trajectories in the underlying image space. We propose 
            INR-V, a video representation network that learns a continuous space for video-based 
            generative tasks. INR-V parameterizes videos using implicit neural representations 
            (INRs), a multi-layered perceptron that predicts an RGB value for each input pixel 
            location of the video. The INR is predicted using a meta-network which is a 
            hypernetwork trained on neural representations of multiple video instances. Later, 
            the meta-network can be sampled to generate diverse novel videos enabling many 
            downstream video-based generative tasks. Interestingly, we find that conditional 
            regularization and progressive weight initialization play a crucial role in obtaining 
            INR-V. The representation space learned by INR-V is more expressive than an image 
            space showcasing many interesting properties not possible with the existing works. 
            For instance, INR-V can smoothly interpolate intermediate videos between known video 
            instances (such as intermediate identities, expressions, and poses in face videos). 
            It can also in-paint missing portions in videos to recover temporally coherent full 
            videos. In this work, we evaluate the space learned by INR-V on diverse generative 
            tasks such as video interpolation, novel video generation, video inversion, and video 
            inpainting against the existing baselines. INR-V significantly outperforms the 
            baselines on several of these demonstrated tasks, clearly showing the potential of 
            the proposed representation space.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    
    </div>
    <!--/ Paper video. -->
    </div>
    </section>
    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src=""
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<section class="overview-section section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <h2 class="headings-scarp">Overview</h2>
        <div class="content">
          <img src="/images/projects/inr-v_tmlr2022/architecture.png" width="65%"/>
        </div>
        </br>
        <div class="content has-text-justified">
          
          <p class="abstract">
            We parameterize videos as a function of space and time using implicit neural representations (INRs). 
            Any point in a video V<sub>hwt</sub> can be represented by a function f<sub>&Theta;</sub>&rarr;
            RGB<sub>hwt</sub> where t denotes the t<sup>th</sup> frame in the video and h, w denote the 
            spatial location in the frame, and RGB denotes the color at the pixel position {h, w, t}. 
            Subsequently, the dynamic dimension of videos (a few million pixels) is reduced to a constant 
            number of weights &Theta; (a few thousand) required for the parameterization. A network can 
            then be used to learn a prior over videos in this parameterized space. This can be obtained 
            through a meta-network that learns a function to map from a latent space to a reduced parameter 
            space that maps to a video. A complete video is thus represented as a single latent point. 
            We use a meta-network called hypernetworks that learns a continuous function over the INRs 
            by getting trained on multiple video instances using a distance loss. However, hypernetworks 
            are notoriously unstable to train, especially on the parameterization of highly expressive 
            signals like videos. Thus, we propose key prior regularization and a progressive weight 
            initialization scheme to stabilize the hypernetwork training allowing it to scale quickly 
            to more than 30,000 videos. The learned prior enables several downstream tasks such as novel 
            video generation, video inversion, future segment prediction, video inpainting, and smooth 
            video interpolation directly at the video level. 
          </p>
          </br>
        </div>
      </div>
    </div>
    <!--/ Method. -->
  </div>
</section>

<section class = "section">

  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-full">
        <!-- <h2 class="headings-scarp">Video Interpolation Comparisons</h2> -->
        <div class="">
          <img src="/images/projects/inr-v_tmlr2022/project_page/comp2.gif" class="interpolation-top" alt="Lights" width="95%">
          <img src="/images/projects/inr-v_tmlr2022/project_page/comp1.gif" class="interpolation-bottom" alt="Lights" width="95%">
        </div>

        <!-- <h2 class="headings-scarp">Video Inversion Results</h2> -->
        <div class="">
          <img src="/images/projects/inr-v_tmlr2022/project_page/inv.gif" class="more-results qual-results results-v1" alt="Lights" width="80%">
        </div>

        <!-- <h2 class="headings-scarp">Inpainting Results</h2> -->
        <div class="">
          <img src="/images/projects/inr-v_tmlr2022/project_page/inpainting.gif" class="more-results qual-results results-v1" alt="Lights" width="80%">
        </div>

        <!-- <h2 class="headings-scarp">Video Inversion Results (Other Tasks)</h2> -->
        <div class="">
          <img src="/images/projects/inr-v_tmlr2022/project_page/inversion-other.gif" class="more-results qual-results results-v1" alt="Lights" width="80%">
        </div>

        <!-- <h2 class="headings-scarp">Video Superresolution Results</h2> -->
        <div class="">
          <img src="/images/projects/inr-v_tmlr2022/project_page/superresolve.gif" class="more-results qual-results results-v1" alt="Lights" width="80%">
        </div>

        <!-- <h2 class="headings-scarp">Results on Additional Datasets</h2> -->
        <div class="">
          <img src="/images/projects/inr-v_tmlr2022/project_page/add1.gif" class="interpolation-top" alt="Lights" width="80%">
          <img src="/images/projects/inr-v_tmlr2022/project_page/add2.gif" class="interpolation-bottom" alt="Lights" width="80%">
        </div>

        <div>
          <h2 class="headings-scarp">Additional Interpolation Results</h2>
          <div class="">
            <img src="/images/projects/inr-v_tmlr2022/project_page/grid-0.gif" class="results-v1" alt="Lights" width="80%">
          </div>
          <br />
          <div class="">
            <img src="/images/projects/inr-v_tmlr2022/project_page/grid-1.gif" class="results-v1" alt="Lights" width="80%">
          </div>
          <br />
          <div class="">
              <img src="/images/projects/inr-v_tmlr2022/project_page/grid-2.gif" class="results-v1" alt="Lights" width="80%">
          </div>
          <br />
          <div class="">
            <img src="/images/projects/inr-v_tmlr2022/project_page/grid-3.gif" class="results-v1" alt="Lights" width="80%">
          </div>
          <br />
          <div class="">
            <img src="/images/projects/inr-v_tmlr2022/project_page/grid-4.gif" class="results-v1" alt="Lights" width="80%">
          </div>
          <br />
          <div class="">
            <img src="/images/projects/inr-v_tmlr2022/project_page/grid-5.gif" class="results-v1" alt="Lights" width="80%">
          </div>
          <br />
          <div class="">
            <img src="/images/projects/inr-v_tmlr2022/project_page/grid-6.gif" class="results-v1" alt="Lights" width="80%">
          </div>
          <br />
          <div class="">
            <img src="/images/projects/inr-v_tmlr2022/project_page/grid-7.gif" class="results-v1" alt="Lights" width="80%">
          </div>
          <br />
          <div class="">
            <img src="/images/projects/inr-v_tmlr2022/project_page/grid-8.gif" class="results-v1" alt="Lights" width="80%">
          </div>
          <br />

        </div>
      </div>
    </div>
  </div>
</section>

<section class="citation-section section">
  <h2 class="ack-scarp headings-scarp">Citation</h2>
  </br>
  <code>
    @article{
      sen2022inrv,<br>
      &nbsp;&nbsp;&nbsp;title={ {INR}-V: A Continuous Representation Space for Video-based Generative Tasks},<br>
      &nbsp;&nbsp;&nbsp;author={Bipasha Sen and Aditya Agarwal and Vinay P Namboodiri and C.V. Jawahar},<br>
      &nbsp;&nbsp;&nbsp;journal={Transactions on Machine Learning Research},<br>
      &nbsp;&nbsp;&nbsp;year={2022},<br>
      &nbsp;&nbsp;&nbsp;url={https://openreview.net/forum?id=aIoEkwc2oB},<br>
      &nbsp;&nbsp;&nbsp;note={}<br>
      }

    <!-- @article{
        2022inrv,<br>
        &nbsp;&nbsp;&nbsp;title={INR-V: A Continuous Representation Space for Videos},<br>
        &nbsp;&nbsp;&nbsp;author={Bipasha Sen and Aditya Agarwal and Vinay P Namboodiri and C. V. Jawahar},<br>
        &nbsp;&nbsp;&nbsp;journal={Transactions on Machine Learning Research},<br>
        &nbsp;&nbsp;&nbsp;year={2022},<br>
        &nbsp;&nbsp;&nbsp;url={https://openreview.net/forum?id=aIoEkwc2oB},<br>
        &nbsp;&nbsp;&nbsp;note={}<br>
        } -->
    <!-- @article{
        anonymous2022inrv,<br>
        &nbsp;&nbsp;&nbsp;title={INR-V: A Continuous Representation Space for Videos},<br>
        &nbsp;&nbsp;&nbsp;author={Anonymous},<br>
        &nbsp;&nbsp;&nbsp;journal={Submitted to Transactions on Machine Learning Research},<br>
        &nbsp;&nbsp;&nbsp;year={2022},<br>
        &nbsp;&nbsp;&nbsp;url={https://openreview.net/forum?id=aIoEkwc2oB},<br>
        &nbsp;&nbsp;&nbsp;note={Under review}<br>
        } -->
</code>
</section>
<br>
<br>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
            It is borrowing the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>
<!-- <script type='text/javascript' src="/bootstrap.js"></script>
<script type="text/javascript" src="/functions.js"></script> -->
<script>
  //starts the carousel
  $('#myCarousel').carousel();
</script>
</body>
</html>
